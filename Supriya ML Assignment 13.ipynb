{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Provide an example of the concepts of Prior, Posterior, and Likelihood.\n",
    "\n",
    "\"\"\"Certainly! Let's consider a classic example involving medical testing: determining whether a patient \n",
    "   has a certain medical condition based on a diagnostic test.\n",
    "\n",
    "   1. Prior:\n",
    "      The prior probability, often denoted as P(A), represents the initial belief or probability of an \n",
    "      event A before taking any new information into account. In our example, let's say a patient walks \n",
    "      into a clinic and we want to estimate the likelihood of them having a certain medical condition, \n",
    "      denoted as \"C\". The prior probability of the patient having the medical condition might be based \n",
    "      on general statistics about the prevalence of the condition in the population, say P(C) = 0.1 (10%).\n",
    "\n",
    "   2. Likelihood:\n",
    "      The likelihood, often denoted as P(B|A), is the probability of observing evidence B given that the \n",
    "      event A has occurred. In our case, let's consider the diagnostic test for the medical condition.\n",
    "      The likelihood represents the probability of getting a positive test result (B) if the patient\n",
    "      actually has the condition (A). Let's assume the likelihood of getting a positive test result given \n",
    "      the patient has the condition is P(B|C) = 0.95 (95%).\n",
    "\n",
    "   3. Posterior:\n",
    "      The posterior probability, often denoted as P(A|B), is the updated probability of event A occurring\n",
    "      after taking new evidence B into account. It's calculated using Bayes' theorem: P(A|B) = (P(B|A) * \n",
    "      P(A)) / P(B). In our example, it's the probability of the patient actually having the medical condition\n",
    "      (A) given that they tested positive (B). This is what we're really interested in. It's calculated by\n",
    "      combining the prior probability, the likelihood, and the probability of getting a positive test result \n",
    "      (irrespective of whether the patient actually has the condition), denoted as P(B).\n",
    "\n",
    "      Let's assume the probability of getting a positive test result regardless of having the condition is\n",
    "      P(B) = 0.15 (15%).\n",
    "\n",
    "      Now, using Bayes' theorem:\n",
    "      P(C|B) = (P(B|C) * P(C)) / P(B)\n",
    "      P(C|B) = (0.95 * 0.1) / 0.15\n",
    "      P(C|B) = 0.6333 (approximately 63.33%)\n",
    "\n",
    "   So, even though the test has a high accuracy (95% true positive rate), the patient's actual probability of\n",
    "   having the condition after a positive test result is still around 63.33%, which is significantly lower than\n",
    "   the initial test accuracy. This demonstrates the importance of considering both the prior information and\n",
    "   the accuracy of the test when interpreting results.\"\"\"\n",
    "\n",
    "#2. What role does Bayes&#39; theorem play in the concept learning principle?\n",
    "\n",
    "\"\"\"Bayes' theorem plays a fundamental role in the concept learning principle, particularly in probabilistic \n",
    "   reasoning and updating beliefs based on new evidence. The concept learning principle involves the process\n",
    "   by which individuals or models learn and update their understanding of concepts or categories based on\n",
    "   observations and evidence.\n",
    "\n",
    "   Bayes' theorem provides a formal framework for updating beliefs or probabilities when new information\n",
    "   becomes available. It helps us incorporate prior knowledge with new evidence to arrive at more accurate \n",
    "   or refined estimates of probabilities. In the context of concept learning, Bayes' theorem allows us to \n",
    "   model how our beliefs about a concept change as we encounter new data or examples.\n",
    "\n",
    "   Here's how Bayes' theorem is relevant to the concept learning principle:\n",
    "\n",
    "   1. Prior Knowledge: Before encountering any new evidence or data, we usually have some prior beliefs or \n",
    "      probabilities about the concepts in question. Bayes' theorem incorporates this prior knowledge into\n",
    "      the learning process.\n",
    "\n",
    "   2. Evidence or Data: As new data or evidence becomes available, it might provide support for or against \n",
    "      our prior beliefs. The likelihood term in Bayes' theorem represents the probability of observing the\n",
    "      evidence given the concept.\n",
    "\n",
    "   3. Updating Beliefs: Bayes' theorem allows us to update our initial beliefs (the prior) based on the \n",
    "      new evidence. The posterior probability, which represents the updated belief, is calculated by \n",
    "      combining the prior with the likelihood.\n",
    "\n",
    "   4. Generalization and Learning: In the context of concept learning, the process involves generalizing \n",
    "      from observed data to make predictions or inferences about new, unseen data. Bayes' theorem helps\n",
    "      us update our beliefs in a way that reflects the accumulated evidence, enabling better generalization\n",
    "      to new instances.\n",
    "\n",
    "   5. Adaptation to New Information: As more evidence accumulates, our beliefs can be refined further.\n",
    "      Bayes' theorem enables a continuous cycle of updating beliefs as new data is collected, leading \n",
    "      to a more accurate understanding of the underlying concepts.\n",
    "\n",
    "  In summary, Bayes' theorem is a powerful tool for modeling how humans and machines learn concepts based\n",
    "  on both prior knowledge and new evidence. It underpins the process of adjusting beliefs as evidence \n",
    "  accumulates, enabling a more accurate and adaptive understanding of the concepts being learned.\"\"\"\n",
    "\n",
    "#3. Offer an example of how the Nave Bayes classifier is used in real life.\n",
    "\n",
    "\"\"\"Sure! One common real-life application of the Naive Bayes classifier is in email categorization or \n",
    "   spam filtering. Naive Bayes is particularly well-suited for text classification tasks like this, \n",
    "   where you want to classify emails into different categories, such as \"spam\" and \"not spam\" (also known as \"ham\").\n",
    "\n",
    "   Here's how the Naive Bayes classifier can be used for email spam filtering:\n",
    "\n",
    "   Problem: You want to automatically classify incoming emails as either spam or not spam based on their content.\n",
    "\n",
    "   Solution:\n",
    "\n",
    "   1. Data Collection: Collect a dataset of labeled emails, where each email is categorized as spam or not\n",
    "      spam. The dataset should include both the email's content (usually represented as words or features)\n",
    "      and its corresponding category label.\n",
    "\n",
    "   2. Feature Extraction: Preprocess the email content to extract relevant features. This could involve \n",
    "      techniques like tokenization (breaking text into words or tokens), removing punctuation, and \n",
    "      converting words to lowercase.\n",
    "\n",
    "   3. Training: Train a Naive Bayes classifier using the labeled dataset. The classifier learns the \n",
    "      statistical relationships between the features (words in this case) and the categories (spam \n",
    "      or not spam). Naive Bayes assumes that the features are conditionally independent given the \n",
    "      category, which is a simplifying assumption.\n",
    "\n",
    "   4. Calculating Probabilities: For each new incoming email, the Naive Bayes classifier calculates \n",
    "      the probabilities of the email belonging to each category (spam or not spam) using Bayes' theorem \n",
    "      and the probabilities learned during training. The class with the highest probability is assigned\n",
    "      as the predicted category for the email.\n",
    "\n",
    "   5. Thresholding: You can set a threshold on the predicted probabilities to make the final decision.\n",
    "      For example, if the probability of an email being spam is above a certain threshold, classify it\n",
    "      as spam; otherwise, classify it as not spam.\n",
    "\n",
    "   6. Feedback Loop: As the classifier is used in practice, it can encounter misclassified emails. \n",
    "      These misclassified emails can be used to improve the classifier through a feedback loop.\n",
    "      The misclassified examples can be added to the training dataset, and the classifier can be\n",
    "      retrained to improve its accuracy over time.\n",
    "\n",
    "   Benefits:\n",
    "\n",
    "   - Efficiency: Naive Bayes is computationally efficient and works well with high-dimensional feature \n",
    "     spaces, making it suitable for processing large volumes of text data like emails.\n",
    "\n",
    "   - Interpretability: The classifier's decision-making process is transparent, as it's based on \n",
    "     probabilities and conditional independence assumptions. This can be useful for understanding \n",
    "     why a certain email was classified as spam.\n",
    "\n",
    "   - Effectiveness: Despite its \"naive\" assumption of feature independence, Naive Bayes often performs \n",
    "     surprisingly well in text classification tasks like spam filtering, especially when you have limited \n",
    "     training data.\n",
    "\n",
    "  Overall, the Naive Bayes classifier is widely used for email categorization and spam filtering due to its \n",
    "  simplicity, efficiency, and effectiveness in handling text data.\"\"\"\n",
    "\n",
    "#4. Can the Nave Bayes classifier be used on continuous numeric data? If so, how can you go about doing it?\n",
    "\n",
    "\"\"\"Yes, the Naive Bayes classifier can be used on continuous numeric data. However, it requires a modification\n",
    "   to the standard Naive Bayes algorithm since the assumption of discrete categorical features doesn't hold \n",
    "   for continuous data. The modified version is often referred to as the Gaussian Naive Bayes classifier.\n",
    "\n",
    "   Here's how you can use the Gaussian Naive Bayes classifier for continuous numeric data:\n",
    "\n",
    "   1. Data Preparation:\n",
    "      You need a dataset with labeled instances where each instance has continuous numeric features and a \n",
    "      corresponding class label. Ensure that the data is properly preprocessed, normalized, and cleaned if necessary.\n",
    "\n",
    "   2. Feature Distribution Assumption:\n",
    "      In the Gaussian Naive Bayes classifier, it's assumed that the features within each class follow a\n",
    "      Gaussian (normal) distribution. This means you need to estimate the mean (average) and standard \n",
    "      deviation (spread) of each feature within each class.\n",
    "\n",
    "   3. Parameter Estimation:\n",
    "      For each class, compute the mean and standard deviation of each feature using the instances belonging to \n",
    "      that class. These parameters will be used to define the Gaussian distribution of each feature within each class.\n",
    "\n",
    "   4. Probability Calculation:\n",
    "      Given a new instance with continuous features, calculate the probability of that instance belonging to\n",
    "      each class. For each feature, use the Gaussian distribution parameters (mean and standard deviation) of \n",
    "      the corresponding class to calculate the likelihood of observing that feature value.\n",
    "\n",
    "   5. Applying Bayes' Theorem:\n",
    "      Combine the probabilities from step 4 with the prior probabilities of each class to calculate the \n",
    "      posterior probabilities using Bayes' theorem. Choose the class with the highest posterior probability\n",
    "      as the predicted class for the new instance.\n",
    "\n",
    "   6. Model Evaluation and Refinement:\n",
    "      As with any classifier, it's important to evaluate the performance of the Gaussian Naive Bayes model on \n",
    "      a separate validation or test dataset. You can use metrics such as accuracy, precision, recall, and\n",
    "      F1-score to assess its performance. Additionally, you might need to fine-tune parameters or handle \n",
    "      cases where features' distributions deviate from Gaussian assumptions.\n",
    "\n",
    "   Advantages of Gaussian Naive Bayes for Continuous Data:\n",
    "\n",
    "   - Efficient: Gaussian Naive Bayes remains computationally efficient, even with continuous features.\n",
    "   - Simplicity: Despite its assumption of Gaussian distribution, it's a relatively simple algorithm to \n",
    "     implement and understand.\n",
    "   - Interpretability: The probability calculations are transparent and allow you to inspect the contributions\n",
    "     of individual features to the classification decision.\n",
    "\n",
    "   Limitations:\n",
    "\n",
    "   - Feature Independence: The \"naive\" assumption of feature independence might not hold in all cases, \n",
    "     especially for highly correlated continuous features.\n",
    "   - Assumption of Gaussian Distribution: The assumption that features follow a Gaussian distribution might \n",
    "     not be valid for all types of data.\n",
    "\n",
    "   In summary, the Gaussian Naive Bayes classifier is an extension of the Naive Bayes algorithm that can be \n",
    "   applied to continuous numeric data. It's a useful option when you want to classify instances with continuous \n",
    "   features while maintaining the benefits of the Naive Bayes approach.\"\"\"\n",
    "\n",
    "#5. What are Bayesian Belief Networks, and how do they work? What are their applications? Are they\n",
    "capable of resolving a wide range of issues?\n",
    "\n",
    "\"\"\"Bayesian Belief Networks (BBNs), also known as Bayesian Networks or Probabilistic Graphical Models, are \n",
    "   a powerful tool for modeling and reasoning under uncertainty. They provide a graphical representation of \n",
    "   probabilistic relationships among a set of variables, along with a formal framework for making inferences\n",
    "   and predictions based on available evidence.\n",
    "\n",
    "   Structure and Working of Bayesian Belief Networks:\n",
    "\n",
    "   A BBN consists of two main components: nodes and edges. Nodes represent variables of interest, and edges \n",
    "   represent probabilistic dependencies between these variables. The structure of the network is often \n",
    "   represented as a directed acyclic graph (DAG), where each node represents a variable and the edges\n",
    "   indicate the dependencies.\n",
    "\n",
    "   The key concept behind BBNs is the use of conditional probability distributions to model the relationships\n",
    "   between variables. Each node's conditional probability distribution is influenced by the values of its\n",
    "   parent nodes, representing the conditional dependencies among variables.\n",
    "\n",
    "   Inference in BBNs involves calculating probabilities for specific events or states given evidence or \n",
    "   observations. The network allows you to update probabilities as new evidence is observed, making it a\n",
    "   powerful tool for decision-making under uncertainty.\n",
    "\n",
    "   Applications of Bayesian Belief Networks:\n",
    "\n",
    "   BBNs find applications in various domains due to their ability to model uncertain, complex relationships\n",
    "   and perform probabilistic reasoning. Some examples of their applications include:\n",
    "\n",
    "   1. Medical Diagnostic: BBNs can assist doctors in diagnosing medical conditions by integrating patient \n",
    "      symptoms, test results, and medical knowledge to provide accurate assessments of different diagnoses.\n",
    "\n",
    "   2. Risk Assessment: BBNs are used to assess risks in fields such as finance, insurance, and engineering. \n",
    "      They can model factors that contribute to risk and help in making informed decisions.\n",
    "\n",
    "   3. Natural Language Processing: BBNs can be applied to language processing tasks, like language understanding \n",
    "      and sentiment analysis, by modeling the probabilistic relationships between words and concepts.\n",
    "\n",
    "   4. Image Recognition: BBNs can aid in image recognition tasks by modeling relationships between image \n",
    "      features and object classes, taking uncertainty into account.\n",
    "\n",
    "   5. Environmental Modeling: BBNs are used to model environmental systems, such as predicting pollution \n",
    "      levels based on factors like weather conditions, industrial activity, and traffic.\n",
    "\n",
    "   6. Fraud Detection: BBNs can be used to detect fraudulent activities by modeling suspicious patterns \n",
    "      and relationships in financial transactions.\n",
    "\n",
    "   Scope and Limitations:\n",
    "\n",
    "   Bayesian Belief Networks are versatile and can handle a wide range of problems involving uncertainty,\n",
    "   provided they can be adequately modeled as probabilistic relationships between variables. However, there \n",
    "   are some limitations:\n",
    "\n",
    "   1. Assumptions of Independence: BBNs assume conditional independence between non-descendant nodes given \n",
    "      their parents, which might not always hold in complex scenarios.\n",
    "\n",
    "   2. Complexity: Building and parameterizing large BBNs can be challenging and computationally expensive.\n",
    "\n",
    "   3. Data Requirements: BBNs require data for parameter estimation and might struggle when data is limited or noisy.\n",
    "\n",
    "   4. Limited to Probabilistic Relationships: BBNs are best suited for problems involving probabilistic\n",
    "      relationships and might not capture more complex causal relationships.\n",
    "\n",
    "  In summary, Bayesian Belief Networks are a powerful tool for modeling uncertain relationships and performing \n",
    "  probabilistic reasoning. They find applications in various domains and can help in decision-making under \n",
    "  uncertainty, but their effectiveness depends on how well the problem can be represented within the probabilistic\n",
    "  framework.\"\"\"\n",
    "\n",
    "#6. Passengers are checked in an airport screening system to see if there is an intruder. Let I be the\n",
    "random variable that indicates whether someone is an intruder I = 1) or not I = 0), and A be the\n",
    "variable that indicates alarm I = 0). If an intruder is detected with probability P(A = 1|I = 1) = 0.98\n",
    "and a non-intruder is detected with probability P(A = 1|I = 0) = 0.001, an alarm will be triggered,\n",
    "implying the error factor. The likelihood of an intruder in the passenger population is P(I = 1) =\n",
    "0.00001. What are the chances that an alarm would be triggered when an individual is actually an\n",
    "intruder?\n",
    "\n",
    "\"\"\"The probability of an alarm being triggered when an individual is actually an intruder can be calculated \n",
    "   using Bayes' theorem. In this case, we want to find the probability of an individual being an intruder \n",
    "   (I = 1) given that an alarm is triggered (A = 1), which is denoted as P(I = 1 | A = 1).\n",
    "\n",
    "   Bayes' theorem states:\n",
    "\n",
    "   \\[ P(I = 1 | A = 1) = \\frac{P(A = 1 | I = 1) \\cdot P(I = 1)}{P(A = 1)} \\]\n",
    "\n",
    "   Given the provided information:\n",
    "\n",
    "   - \\( P(A = 1 | I = 1) = 0.98 \\) (Probability of an alarm being triggered when an individual is an intruder)\n",
    "   - \\( P(I = 1) = 0.00001 \\) (Likelihood of an intruder in the passenger population)\n",
    "\n",
    "   We need to calculate \\( P(A = 1) \\), the probability of an alarm being triggered (regardless of whether\n",
    "   an individual is an intruder or not). This can be calculated by considering both cases: an alarm triggered\n",
    "   by an intruder and an alarm triggered by a non-intruder.\n",
    "\n",
    "   - An alarm can be triggered by an intruder with a probability of \\( P(A = 1 | I = 1) = 0.98 \\).\n",
    "   - An alarm can be triggered by a non-intruder with a probability of \\( P(A = 1 | I = 0) = 0.001 \\).\n",
    "\n",
    "   Since there are only two possibilities (intruder or non-intruder), the total probability of an alarm being \n",
    "   triggered is the sum of the probabilities for each case:\n",
    "\n",
    "   \\[ P(A = 1) = P(A = 1 | I = 1) \\cdot P(I = 1) + P(A = 1 | I = 0) \\cdot P(I = 0) \\]\n",
    "\n",
    "   Given that \\( P(I = 0) = 1 - P(I = 1) \\), and in this case, \\( P(I = 0) = 1 - 0.00001 = 0.99999 \\), we can \n",
    "   calculate \\( P(A = 1) \\):\n",
    "\n",
    "   \\[ P(A = 1) = 0.98 \\cdot 0.00001 + 0.001 \\cdot 0.99999 \\]\n",
    "\n",
    "   Finally, we can use Bayes' theorem to find \\( P(I = 1 | A = 1) \\):\n",
    "\n",
    "   \\[ P(I = 1 | A = 1) = \\frac{P(A = 1 | I = 1) \\cdot P(I = 1)}{P(A = 1)} \\]\n",
    "\n",
    "   Plug in the values:\n",
    "\n",
    "   \\[ P(I = 1 | A = 1) = \\frac{0.98 \\cdot 0.00001}{P(A = 1)} \\]\n",
    "\n",
    "   \\[ P(I = 1 | A = 1) = \\frac{0.0000098}{P(A = 1)} \\]\n",
    "\n",
    "   To find the exact value of \\( P(I = 1 | A = 1) \\), you would need to calculate \\( P(A = 1) \\) and then \n",
    "   substitute it into the equation.\"\"\"\n",
    "\n",
    "#7. An antibiotic resistance test (random variable T) has 1% false positives (i.e., 1% of those who are\n",
    "not immune to an antibiotic display a positive result in the test) and 5% false negatives (i.e., 1% of\n",
    "those who are not resistant to an antibiotic show a positive result in the test) (i.e. 5 percent of those\n",
    "actually resistant to an antibiotic test negative). Assume that 2% of those who were screened were\n",
    "antibiotic-resistant. Calculate the likelihood that a person who tests positive is actually immune\n",
    "(random variable D).\n",
    "\n",
    "\"\"\"Let's use the given information to calculate the likelihood that a person who tests positive is actually\n",
    "   immune (antibiotic-resistant). In this context, the variables are defined as follows:\n",
    "\n",
    "   - T: Antibiotic resistance test result (positive or negative)\n",
    "   - D: Immunity to an antibiotic (immune or not immune)\n",
    "\n",
    "   We want to calculate \\(P(D = \\text{immune} | T = \\text{positive})\\), which represents the probability that\n",
    "   a person is immune to an antibiotic given that they tested positive in the antibiotic resistance test.\n",
    "\n",
    "   We can use Bayes' theorem to calculate this probability:\n",
    "\n",
    "   \\[ P(D = \\text{immune} | T = \\text{positive}) = \\frac{P(T = \\text{positive} | D = \\text{immune}) \\cdot \n",
    "   P(D = \\text{immune})}{P(T = \\text{positive})} \\]\n",
    "\n",
    "   Given the information:\n",
    "\n",
    "   - \\( P(T = \\text{positive} | D = \\text{immune}) = 1 - \\text{false\\_negatives} = 1 - 0.05 = 0.95 \\)\n",
    "   (Probability of testing positive if actually immune)\n",
    "   - \\( P(D = \\text{immune}) = 0.02 \\) (Probability of being immune)\n",
    "   - \\( P(T = \\text{positive}) \\) can be calculated using the law of total probability:\n",
    "\n",
    "   \\[ P(T = \\text{positive}) = P(T = \\text{positive} | D = \\text{immune}) \\cdot P(D = \\text{immune}) + P(T\n",
    "   = \\text{positive} | D = \\text{not immune}) \\cdot P(D = \\text{not immune}) \\]\n",
    "\n",
    "   Given that \\( P(T = \\text{positive} | D = \\text{immune}) = 1 - \\text{false\\_negatives} = 0.95 \\)\n",
    "   (Probability of testing positive if actually immune)\n",
    "   and \\( P(D = \\text{not immune}) = 1 - P(D = \\text{immune}) = 1 - 0.02 = 0.98 \\),\n",
    "\n",
    "   we need to calculate \\( P(T = \\text{positive} | D = \\text{not immune}) \\):\n",
    "   - \\( P(T = \\text{positive} | D = \\text{not immune}) = \\text{false\\_positives} = 0.01 \\) (Probability of \n",
    "   testing positive if not immune)\n",
    "\n",
    "   Substitute these values into the calculation for \\( P(T = \\text{positive}) \\):\n",
    "\n",
    "   \\[ P(T = \\text{positive}) = 0.95 \\cdot 0.02 + 0.01 \\cdot 0.98 \\]\n",
    "\n",
    "   Finally, plug the values into the Bayes' theorem formula:\n",
    "\n",
    "   \\[ P(D = \\text{immune} | T = \\text{positive}) = \\frac{0.95 \\cdot 0.02}{P(T = \\text{positive})} \\]\n",
    "\n",
    "   Calculate \\( P(D = \\text{immune} | T = \\text{positive}) \\) using the values obtained. This will give you the\n",
    "   likelihood that a person who tests positive is actually immune to the antibiotic.\"\"\"\n",
    "\n",
    "#8. In order to prepare for the test, a student knows that there will be one question in the exam that\n",
    "is either form A, B, or C. The chances of getting an A, B, or C on the exam are 30 percent, 20%, and\n",
    "50 percent, respectively. During the planning, the student solved 9 of 10 type A problems, 2 of 10\n",
    "type B problems, and 6 of 10 type C problems.\n",
    "\n",
    "1. What is the likelihood that the student can solve the exam problem?\n",
    "\n",
    "2. Given the student's solution, what is the likelihood that the problem was of form A?\n",
    "\n",
    "\"\"\"To answer these questions, we can use Bayes' theorem, which relates conditional probabilities.\n",
    "   Let's define some variables:\n",
    "\n",
    "   - Let event A be the student solving the exam problem.\n",
    "   - Let event B be the problem being of form A, B, or C.\n",
    "\n",
    "   We are given the following probabilities:\n",
    "\n",
    "   - P(A|B) = Probability of solving the exam problem given that it's of form A.\n",
    "   - P(A|C) = Probability of solving the exam problem given that it's of form C.\n",
    "   - P(A|B') = Probability of solving the exam problem given that it's not of form A (i.e., either form B or C).\n",
    "\n",
    "   We are also given the probabilities of the problem being of each form:\n",
    "\n",
    "   - P(B) = Probability of the problem being of form A.\n",
    "   - P(C) = Probability of the problem being of form C.\n",
    "   - P(B') = Probability of the problem not being of form A (i.e., either form B or C).\n",
    "\n",
    "   We are asked to find:\n",
    "\n",
    "   1. P(A) = Likelihood that the student can solve the exam problem.\n",
    "   2. P(B|A) = Likelihood that the problem was of form A given the student's solution.\n",
    "\n",
    "   Using Bayes' theorem:\n",
    "\n",
    "   1. P(A) = P(A|B) * P(B) + P(A|C) * P(C) + P(A|B') * P(B')\n",
    "   2. P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "   Given the following probabilities:\n",
    "   - P(A|B) = 9/10 (solving 9 out of 10 type A problems)\n",
    "   - P(A|C) = 6/10 (solving 6 out of 10 type C problems)\n",
    "   - P(A|B') = 2/10 (solving 2 out of 10 type B problems)\n",
    "   - P(B) = 0.3 (30%)\n",
    "   - P(C) = 0.5 (50%)\n",
    "   - P(B') = 1 - P(B) = 0.7\n",
    "\n",
    "   Let's plug these values into the formulas:\n",
    "\n",
    "   1. P(A) = (9/10 * 0.3) + (6/10 * 0.5) + (2/10 * 0.7) ≈ 0.54\n",
    "\n",
    "   2. P(B|A) = (9/10 * 0.3) / 0.54 ≈ 0.5\n",
    "\n",
    "   So, the answers are approximately:\n",
    "\n",
    "   1. The likelihood that the student can solve the exam problem is 0.54 (or 54%).\n",
    "   2. The likelihood that the problem was of form A given the student's solution is 0.5 (or 50%).\"\"\"\n",
    "\n",
    "#9. A bank installs a CCTV system to track and photograph incoming customers. Despite the constant\n",
    "influx of customers, we divide the timeline into 5 minute bins. There may be a customer coming into\n",
    "the bank with a 5% chance in each 5-minute time period, or there may be no customer (again, for\n",
    "simplicity, we assume that either there is 1 customer or none, not the case of multiple customers). If\n",
    "\n",
    "there is a client, the CCTV will detect them with a 99 percent probability. If there is no customer, the\n",
    "camera can take a false photograph with a 10% chance of detecting movement from other objects.\n",
    "\n",
    "1. How many customers come into the bank on a daily basis (10 hours)?\n",
    "\n",
    "2. On a daily basis, how many fake photographs (photographs taken when there is no\n",
    "customer) and how many missed photographs (photographs taken when there is a customer) are\n",
    "there?\n",
    "\n",
    "3. Explain likelihood that there is a customer if there is a photograph?\n",
    "\n",
    "\"\"\"Let's break down the problem step by step:\n",
    "\n",
    "   1. Number of Customers on a Daily Basis:\n",
    "      The bank operates for 10 hours, which is 600 minutes (10 hours * 60 minutes/hour). In each 5-minute\n",
    "      interval, there's a 5% chance of a customer coming in. So, on average, in each 5-minute interval,\n",
    "      we would expect 0.05 customers. In 600 minutes, there are 120 intervals (600 minutes / 5 minutes/interval),\n",
    "      so we can expect:\n",
    "\n",
    "    Number of customers = 120 intervals * 0.05 customers/interval = 6 customers on average per day.\n",
    "\n",
    "   2. Fake and Missed Photographs:\n",
    "      For every 5-minute interval, there are two possibilities: either a customer comes in or doesn't.\n",
    "      Let's calculate the probabilities:\n",
    "\n",
    "      - Probability of a customer coming in: 0.05\n",
    "      - Probability of no customer coming in: 1 - 0.05 = 0.95\n",
    "\n",
    "   For a customer coming in, the CCTV detects them with 99% probability, so there's a 1% chance of a\n",
    "   missed photograph.\n",
    "   For no customer coming in, the CCTV takes a false photograph with a 10% chance of detecting movement\n",
    "   from other objects.\n",
    "\n",
    "   Expected number of missed photographs = 120 intervals * 0.05 customers/interval * 0.01 missed/customer = \n",
    "   0.6 missed photographs on average per day.\n",
    "\n",
    "   Expected number of fake photographs = 120 intervals * 0.95 no customers/interval * 0.1 false alarms/no \n",
    "   customer = 11.4 fake photographs on average per day.\n",
    "\n",
    "   3. Likelihood of Customer Given a Photograph:\n",
    "      This is a conditional probability problem. We want to find the likelihood that there is a customer \n",
    "      given that a photograph was taken. Let's define events:\n",
    "\n",
    "    - A: There is a customer.\n",
    "    - B: A photograph was taken.\n",
    "\n",
    "    We are looking for P(A|B), which can be calculated using Bayes' theorem:\n",
    "\n",
    "    P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "   - P(B|A) = Probability of a photograph being taken when there is a customer = 1 (CCTV detects customers \n",
    "     with 99% probability).\n",
    "   - P(A) = Probability of a customer coming in = 0.05.\n",
    "   - P(B) = Probability of a photograph being taken.\n",
    "\n",
    "   For a photograph to be taken, there are two possibilities: a customer comes in and a photograph is taken, \n",
    "   or no customer comes in and a false photograph is taken.\n",
    "\n",
    "   P(B) = P(A) * P(B|A) + P(not A) * P(B|not A)\n",
    "        = 0.05 * 1 + 0.95 * 0.1\n",
    "        = 0.145\n",
    "\n",
    "   Now we can calculate:\n",
    "\n",
    "   P(A|B) = P(B|A) * P(A) / P(B)\n",
    "          = 1 * 0.05 / 0.145\n",
    "          ≈ 0.345\n",
    "\n",
    "   So, the likelihood that there is a customer if there is a photograph is approximately 0.345 (or 34.5%).\"\"\"\n",
    "\n",
    "#10. Create the conditional probability table associated with the node Won Toss in the Bayesian Belief\n",
    "network to represent the conditional independence assumptions of the Nave Bayes classifier for the\n",
    "match winning prediction problem in Section 6.4.4.\n",
    "\n",
    "\"\"\"It looks like you're referring to a specific section or example from a resource that I don't have direct\n",
    "   access to, as my training only includes information up until September 2021 and I don't have access to \n",
    "   external content like Section 6.4.4. However, I can guide you on how to create a conditional probability\n",
    "   table (CPT) for the \"Won Toss\" node in a Naive Bayes classifier.\n",
    "\n",
    "   In a Naive Bayes classifier, the assumption is that all features are conditionally independent given the \n",
    "   class label. Let's assume you have a binary classification problem where you're predicting whether a team\n",
    "   will win a match or not based on features like \"Won Toss\" and other features (e.g., \"Weather,\" \"Team \n",
    "   Strength,\" etc.). The \"Won Toss\" feature is binary (Yes/No), and the class label is also binary (Win/Lose).\n",
    "\n",
    "   Here's how you can create a CPT for the \"Won Toss\" node:\n",
    "\n",
    "```\n",
    "| Won Toss | P(Won Toss) |\n",
    "|----------|-------------|\n",
    "| Yes      | P(Yes)      |\n",
    "| No       | P(No)       |\n",
    "```\n",
    "\n",
    "   In this table, you would fill in the probabilities P(Yes) and P(No), which are the probabilities of winning \n",
    "   the match given that the team won the toss and the probability of winning the match given that the team did \n",
    "   not win the toss, respectively. These probabilities can be estimated from your training data.\n",
    "\n",
    "   Remember that in a Naive Bayes classifier, the assumption is that the features are conditionally independent\n",
    "   given the class label. So, the \"Won Toss\" feature's probability is directly related to the class label probabilities.\n",
    "\n",
    "   Please adapt the table based on the specifics of your problem and the information you have. If you can provide \n",
    "   more details about the context and the variables involved, I can help you create a more specific CPT.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
